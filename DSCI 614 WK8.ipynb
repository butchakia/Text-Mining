{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 614 Text Mining\n",
    "# Week 8: Topic Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectives**\n",
    "\n",
    "\n",
    "After you complete this module, students will be able to:\n",
    "\n",
    "+ Clean and preprocess the texts.\n",
    "+ Perform Latent Dirichlet Allocation (LDA) algorithm based on CountVectorizer.\n",
    "+ Perform Latent Dirichlet Allocation (LDA) algorithm based on TfidfVectorizer.\n",
    "+ Find the top words for each topic and visualize them.\n",
    "+ Visualize the topic Model using pyLDAvis.\n",
    "\n",
    "\n",
    "We covered the text classification that is supervised learning in the previous week. We will cover the topic model this week. It is an unsupervised machine learning method without a label for text data. First, we will load and clean the texts. Secondly,  I will show you how to build LDA models using sklearn. Second, we will visualie the top words for each topic. Finally, we will cover how to plot the topic models using dimensiona reduction techinques such as PCA and t-SNE.\n",
    "\n",
    "\n",
    "\n",
    "**Readings**\n",
    "\n",
    "+ Latent Dirichlet allocation (https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
    "+ Latent Dirichlet Allocation with online variational Bayes algorithm (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)\n",
    "+ PyLDAvis: Python library for interactive topic model visualization (https://github.com/bmabey/pyLDAvis)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# The Twitter US Airline \n",
    "\n",
    "You have given a Twitter US Airline sentiment dataset containing in Tweets.csv (https://www.kaggle.com/crowdflower/twitter-airline-sentiment/), which is a comma-separated file. It has the following columns:\n",
    "\n",
    "+ tweet_id\n",
    "+ airline_sentiment\n",
    "+ airline_sentiment_confidence\n",
    "+ negativereason\n",
    "+ negativereason_confidence\n",
    "+ airline\n",
    "+ airline_sentiment_gold\n",
    "+ name\n",
    "+ negativereason_gold\n",
    "+ retweet_count\n",
    "+ text\n",
    "+ tweet_coord\n",
    "+ tweet_created\n",
    "+ tweet_location\n",
    "+ user_timezone\n",
    "\n",
    "This project aims to **build topic models on the text data without the label**.\n",
    " \n",
    " \n",
    "Let's load the dataset into memory using the pandas library. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "twitter_airline=pd.read_csv('Tweets.csv')\n",
    "twitter_airline.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select all the text features and the label from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_airline = twitter_airline[[\"airline\", \"airline_sentiment\", \"negativereason\", \"airline_sentiment_gold\", \"negativereason_gold\", \"text\" ]]\n",
    "twitter_airline = twitter_airline[[\"airline\", \"negativereason\", \"airline_sentiment_gold\", \"negativereason_gold\", \"text\" ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the missing values in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 5 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   airline                 14640 non-null  object\n",
      " 1   negativereason          9178 non-null   object\n",
      " 2   airline_sentiment_gold  40 non-null     object\n",
      " 3   negativereason_gold     32 non-null     object\n",
      " 4   text                    14640 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 572.0+ KB\n"
     ]
    }
   ],
   "source": [
    "twitter_airline.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline                    0.000000\n",
       "negativereason            37.308743\n",
       "airline_sentiment_gold    99.726776\n",
       "negativereason_gold       99.781421\n",
       "text                       0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_missing = twitter_airline.isnull().sum() * 100 / len(twitter_airline)\n",
    "percent_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's concatenate all the texts into the feature of text_review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = [\"negativereason\", \"airline_sentiment_gold\", \"negativereason_gold\", \"text\" ]\n",
    "twitter_airline['text_review'] = twitter_airline[text_cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select all the columns to perform our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_airline = twitter_airline[['airline', 'text_review']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to clean the text_review column using regular expression and Spacy library. \n",
    "We first remove nan, @airline, punctuation, URL, or any non-alphanumeric characters and separate word using a single space. Then we remove stop words and obtain the lemma of the tokens/words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Load the small model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Creating our tokenzer function from a given sentence/review\n",
    "def clean_text(sentence):\n",
    "    # Remove nan, @airline, punctuation, URL, or any non alpanumeric characters and seperate word using a single space.\n",
    "    sentence = ' '.join(re.sub(\"(nan)|(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \", sentence).split())\n",
    "    # Removing stop words and obtain the lemma\n",
    "    text = [ word.lemma_ for word in nlp(sentence) if word not in stop_words]\n",
    "    return ' '.join(text).strip().lower()\n",
    "\n",
    "# Apply clean_text function to the column.\n",
    "twitter_airline['text_review_cleaned'] = twitter_airline['text_review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 3 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   airline              14640 non-null  object\n",
      " 1   text_review          14640 non-null  object\n",
      " 2   text_review_cleaned  14640 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 343.3+ KB\n"
     ]
    }
   ],
   "source": [
    "twitter_airline.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline                0.0\n",
       "text_review            0.0\n",
       "text_review_cleaned    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_missing = twitter_airline.isnull().sum() * 100 / len(twitter_airline)\n",
    "percent_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>text_review</th>\n",
       "      <th>text_review_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>nan nan nan @VirginAmerica What @dhepburn said.</td>\n",
       "      <td>what say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>nan nan nan @VirginAmerica plus you've added c...</td>\n",
       "      <td>plus you ve add commercial to the experience t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>nan nan nan @VirginAmerica I didn't today... M...</td>\n",
       "      <td>i didn t today must mean i need to take anothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>Bad Flight nan nan @VirginAmerica it's really ...</td>\n",
       "      <td>bad flight it s really aggressive to blast obn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virgin America</td>\n",
       "      <td>Can't Tell nan nan @VirginAmerica and it's a r...</td>\n",
       "      <td>can t tell and it s a really big bad thing abo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          airline                                        text_review  \\\n",
       "0  Virgin America    nan nan nan @VirginAmerica What @dhepburn said.   \n",
       "1  Virgin America  nan nan nan @VirginAmerica plus you've added c...   \n",
       "2  Virgin America  nan nan nan @VirginAmerica I didn't today... M...   \n",
       "3  Virgin America  Bad Flight nan nan @VirginAmerica it's really ...   \n",
       "4  Virgin America  Can't Tell nan nan @VirginAmerica and it's a r...   \n",
       "\n",
       "                                 text_review_cleaned  \n",
       "0                                           what say  \n",
       "1  plus you ve add commercial to the experience t...  \n",
       "2  i didn t today must mean i need to take anothe...  \n",
       "3  bad flight it s really aggressive to blast obn...  \n",
       "4  can t tell and it s a really big bad thing abo...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_airline.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA) Algorithm based on CountVectorizer\n",
    "\n",
    "The Latent Dirichlet Allocation (LDA) is a generative probabilistic model for a collection of text documents such as a corpus. It is used the Bayesian approach and EM algorithm to estate the parameters. Please read the paper: Latent Dirichlet Allocation (https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) for detailed information.\n",
    "\n",
    "Let's develop our first Latent Dirichlet Allocation (LDA) Algorithm based on CountVectorizer. To perform LDA, we first need to construct a document word matrix using either CountVectorizer or TfidfVectorizer.\n",
    "\n",
    "When we create a document word matrix, there are many hyper parameters to set. Please see the help document (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for detailed information. For illustration purposes, we set the following hyperparameters:\n",
    "\n",
    "+ **max_df**: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "+ **min_df**: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "+ **stop_words** : {‘english’}, list, default=None\n",
    "If ‘english’, a built-in stop word list for English is used. There are several known issues with ‘english’ and you should consider an alternative (see Using stop words). If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word'. If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer(max_df=0.90, min_df=5, stop_words='english')\n",
    "# Create document word matrix\n",
    "document_word_matrix_tf = tf_vectorizer.fit_transform(twitter_airline['text_review_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we generate the document word matrix, we can perform LDA analysis on it. Latent Dirichlet Allocation algorithm has several parameters to specify. Please see the official help document (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) for detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=4, random_state=101)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=4, random_state=101)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(n_components=4, random_state=101)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Specify the number of topics using n_components and fix the seed\n",
    "lda = LatentDirichletAllocation(n_components= 4 ,random_state= 101)\n",
    "lda.fit(document_word_matrix_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Words for Each Topic\n",
    "\n",
    "LDA algorithm has one crucial attribute of **components_**. It is a two-dimensional vector. I summarize it as follows:\n",
    "\n",
    "+ The row denotes the topics. We specified four topics. Therefore it has four rows.\n",
    "+ The column represents the document words generated from either CountVectorizer or TfidfVectorizer.\n",
    "\n",
    "It can also be viewed as a distribution over the words for each topic after normalization. Therefore the top words have the largest values/probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(tf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names())\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "len(tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print our the top 15 words for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 15 \n",
    "# Loop over each topic\n",
    "for index,topic in enumerate(lda.components_):\n",
    "    print(f'THE TOP {n_top_words} WORDS FOR TOPIC #{index}')\n",
    "    # Get the top word for a given topics using list comprehension\n",
    "    # We rank the values using argsort and take out the corresponding indices\n",
    "    # Finally obtain the corresponding feature names (word) using the indices\n",
    "    print([tf_vectorizer.get_feature_names()[j] for j in topic.argsort()[-n_top_words:]])\n",
    "    #Seperate each topic using #\n",
    "    print('#'*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to visualize the top 15 words for each topic. The following function is taken from Scikit learn official help document(https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    # We specify 4 topics, then we setup the number of subplot =2*2 \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(30, 15), sharex=True)\n",
    "    # Flatten the axes to 1 D vector, then we can loop it easily instead of using a nested loop for rows and columns\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop over all the topics\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        # Rank the values and take out the corresponding indices\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        # Take out the top features using the indices\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        # Get the weights\n",
    "        weights = topic[top_features_ind]\n",
    "        # Take out hte axes from the flattened 1D vector\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, 'Topics in LDA model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign topics ID to Each Document\n",
    "\n",
    "Let us transform the data of document_word_matrix according to the fitted model. The result is a 2-dimensional vector. The row denotes the document/review, and the column is the topic ID. The corresponding values are the probabilities belonging to each topic for a given document/review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.transform(document_word_matrix_tf)\n",
    "topics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the probabilities of each topic for the first document/review. It is easy for us to see that.\n",
    "\n",
    "+ The sum of these probabilities is 1.\n",
    "+ The largest number is the 4th number. Therefore, it is assigned to the 4th topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we assign the topic ID to each document/review by taking out the index of the largest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column and save the predicted topic ID\n",
    "twitter_airline['Topic'] = topics.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_airline['Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling Visualization using pyLDAvis\n",
    "\n",
    "According to the official GitHub of the pyLDAvis:\n",
    "\n",
    "pyLDAvis is a  Python library for interactive topic model visualization. It is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization. The visualization is intended to be used within an IPython notebook but can also be saved to a stand-alone HTML file for easy sharing.\n",
    "\n",
    "https://github.com/bmabey/pyLDAvis\n",
    "\n",
    "To use pyLDAvis, we need to install it in the Anaconda Prompt using the following command:\n",
    "\n",
    "<center>  pip install pyldavis </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA) Algorithm using TfidfVectorizer\n",
    "We are using the TfidfVectorizer to create the Latent Dirichlet Allocation (LDA) model in this section. It is very similar to the Latent Dirichlet Allocation (LDA) Algorithm based on CountVectorizer before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "\n",
    "# Generate the document word matrix using tfidf\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = 'english', # Remove the stop words\n",
    "                                lowercase = True, # Convert the tokes to lower cases\n",
    "                                # Specif the token using regular expression to consider tokens of 2 for more alphanumeric characters\n",
    "                                token_pattern = r'\\b[a-zA-Z0-9]{2,}\\b', \n",
    "                                max_df = 0.9, \n",
    "                                min_df = 5)\n",
    "document_word_matrix_tf_idf = tfidf_vectorizer.fit_transform(twitter_airline['text_review_cleaned'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the LDA model\n",
    "lda_tf_idf = LatentDirichletAllocation(n_components= 4, random_state= 101)\n",
    "lda_tf_idf.fit(document_word_matrix_tf_idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Topic Modeling using pyLDAvis\n",
    "\n",
    "PyLDAvis is a port of the fabulous R package of LDAvis by Carson Sievert and Kenny Shirley. LDAvis is summarized in the paper LDAvis: A method for visualizing and interpreting topics. (https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf).\n",
    "\n",
    "We are using pyLDAvis.sklearn.prepare function to plot it. This funciton has three important parameters as follows:\n",
    "\n",
    "+ **lda_model** : sklearn.decomposition.LatentDirichletAllocation.\n",
    "        Latent Dirichlet Allocation model from sklearn fitted with `dtm`\n",
    "+ **dtm** : array-like or sparse matrix, shape=(n_samples, n_features)\n",
    "        Document-term matrix used to fit on LatentDirichletAllocation model (`lda_model`)\n",
    "+ **vectorizer** : sklearn.feature_extraction.text.(**CountVectorizer, TfIdfVectorizer**).\n",
    "        vectorizer used to convert raw documents to document-term matrix (`dtm`)\n",
    "+ **mds** :function or a string representation of function. A function that takes topic_term_dists as an input and outputs a n_topics by 2 distance matrix. The output approximates the distance between topics. See js_PCoA() for details on the default function. A string representation currently accepts:\n",
    "    + **pcoa** (or upper case variant) \n",
    "    + **mmds** (or upper case variant), if sklearn package is installed \n",
    "    + **tsne** (or upper case variant), if sklearn package is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "# Enable automatic D3 display of prepared model data in the IPython notebook.\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Visualize topic model with default mds='pcoa'\n",
    "pyLDAvis.sklearn.prepare(lda_model = lda_tf_idf, dtm = document_word_matrix_tf_idf, vectorizer = tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**The left panel**, labeled Intertopic Distance Map:\n",
    "+ Circles represent different topics and the distance between them. Similar topics appear closer and dissimilar topics farther. \n",
    "+ The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus. \n",
    "+ An individual topic may be selected for closer scrutiny by clicking on its circle or entering its number in the \"selected topic\" box in the upper-left.\n",
    "\n",
    "**The right panel**, including the bar chart of the top 30 terms. \n",
    "+ When no topic is selected in the plot on the left, the bar chart shows the top-30 most \"salient\" terms in the corpus. \n",
    "+ A term's saliency is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics. Global view before we select any topic( in other words, it consider all the topics0\n",
    "$$ \\text{saliency(term w)}= \\text{freqency(w)}*\\sum_tp(t|w)*log(p(t|w)/p(t))$$\n",
    "+ Selecting each topic on the left, modifies the bar chart to show the \"relevant\" terms for the selected topic. It is used to rank\n",
    "terms within topics.\n",
    "Relevance is defined as in footer 2. It is the local view for a given topic\n",
    "\n",
    "$$ \\text{relevance(term w| topic t)}= \\lambda*p(w|t)+(1-\\lambda)*p(w|t)/p(w)$$\n",
    "\n",
    "Where $ \\lambda$ determines the weight given to the probability of term w under topic t relative to its lift\n",
    "(measuring both on the log scale). Setting $\\lambda =1$\n",
    "results in the familiar ranking of terms in decreasing order of their topic-specific probability, and\n",
    "setting $\\lambda =0$  ranks terms solely by their lift.\n",
    "\n",
    "It can be tuned by the parameter $\\lambda$.\n",
    "+ Smaller $\\lambda$, gives higher weight to the term's distinctiveness \n",
    "+ While larger $\\lambda$, corresponds to the probability of the term occurrence per topic.\n",
    "\n",
    "Therefore, to get a better sense of terms per topic, we'll use $\\lambda =0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Different MDS Functions\n",
    "\n",
    "With sklearn installed, other MDS functions, such as MMDS and TSNE can be used for plotting if the default PCoA is not satisfactory.\n",
    "\n",
    "\n",
    "+ **Multi-dimensional Scaling (MDS)**\n",
    "\n",
    "    https://scikit-learn.org/stable/modules/manifold.html#multidimensional-scaling\n",
    "\n",
    "+ **t-distributed Stochastic Neighbor Embedding**\n",
    "\n",
    "    https://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_model = lda_tf_idf, dtm = document_word_matrix_tf_idf, vectorizer = tfidf_vectorizer, mds='mmds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.sklearn.prepare(lda_model = lda_tf_idf, dtm = document_word_matrix_tf_idf, vectorizer = tfidf_vectorizer, mds='tsne')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "+ We create a topic model using LDA and CountVectorizer.\n",
    "+ We create a topic model using LDA and TfidfVectorizer.\n",
    "+ We visualize the top words in each topic.\n",
    "+ We visualize topic modeling using pyLDAvis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
